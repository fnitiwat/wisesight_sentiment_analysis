{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Generation From Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details how we can generate extra training data by using the first few words of a text as seed and generate variable-length sequences of a minority/often-misclassified class `pos` and `neg` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "# ulmfit\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import CSVLogger, SaveModelCallback\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "model_path = \"wisesight_data/\"\n",
    "\n",
    "\n",
    "def replace_url(text):\n",
    "    URL_PATTERN = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    return re.sub(URL_PATTERN, 'xxurl', text)\n",
    "\n",
    "\n",
    "def replace_rep(text):\n",
    "    def _replace_rep(m):\n",
    "        c, cc = m.groups()\n",
    "        return f\"{c}xxrep\"\n",
    "    re_rep = re.compile(r\"(\\S)(\\1{2,})\")\n",
    "\n",
    "    return re_rep.sub(_replace_rep, text)\n",
    "\n",
    "\n",
    "def ungroup_emoji(toks):\n",
    "    res = []\n",
    "    for tok in toks:\n",
    "        if emoji.emoji_count(tok) == len(tok):\n",
    "            for char in tok:\n",
    "                res.append(char)\n",
    "        else:\n",
    "            res.append(tok)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    #pre rules\n",
    "    res = text.lower().strip()\n",
    "    res = replace_url(res)\n",
    "    res = replace_rep(res)\n",
    "    \n",
    "    #tokenize\n",
    "    res = [word for word in word_tokenize(res, engine=\"ulmfit\") if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
    "    \n",
    "    #post rules\n",
    "    res = ungroup_emoji(res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"all_df.csv\")\n",
    "\n",
    "all_df[\"processed\"] = all_df.texts.map(lambda x: \"|\".join(process_text(x)))\n",
    "all_df[\"wc\"] = all_df.processed.map(lambda x: len(x.split(\"|\")))\n",
    "all_df[\"uwc\"] = all_df.processed.map(lambda x: len(set(x.split(\"|\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    14243\n",
       "neg     5713\n",
       "pos     3917\n",
       "q        472\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevalence; let's generate like 2,000 of them\n",
    "all_df.category.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5191.000000\n",
       "mean       10.681757\n",
       "std         4.442878\n",
       "min         5.000000\n",
       "25%         7.000000\n",
       "50%        10.000000\n",
       "75%        14.000000\n",
       "max        20.000000\n",
       "Name: wc, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_df = all_df[((all_df.category==\"pos\")|(all_df.category==\"neg\"))&((all_df.wc>=5)&(all_df.wc<=20))].reset_index(drop=True)\n",
    "seed_df.wc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm data\n",
    "data_lm = load_data(model_path, \"wisesight_lm.pkl\")\n",
    "data_lm.sanity_check()\n",
    "\n",
    "# learner\n",
    "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
    "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
    "trn_args = dict(drop_mult=1., clip=0.12, alpha=2, beta=1)\n",
    "\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
    "\n",
    "#load pretrained models\n",
    "learn.load(\"wisesight_lm\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6eb072994344d20b11adee84f2bd143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate fake data\n",
    "new_rows = []\n",
    "for i,row in tqdm_notebook(seed_df.iterrows()):\n",
    "    seed_words = \"\".join(row[\"processed\"].split(\"|\")[:3])\n",
    "    n_words = np.random.randint(5,21)\n",
    "\n",
    "    new_texts = learn.predict(seed_words, n_words=n_words, temperature=1.0, sep=\"\")\n",
    "    new_rows.append({\"category\": row[\"category\"], \"texts\": new_texts, \"processed\": \"\", \"wc\": n_words, \"uwc\": 0})\n",
    "\n",
    "fake_df = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>texts</th>\n",
       "      <th>processed</th>\n",
       "      <th>wc</th>\n",
       "      <th>uwc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>ก็น่าจะอร่อยกว่าบาบีก้อนอีกเลย</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>อ่านเม้นดิน.มีปัญหามีปัญหามีปัญหานะนะนะ   คุณค...</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>โบกแท็กซี่สิบคันกินข้าวกับเพื่อนไม่ได้</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>ท่าทางยาบ้าเสียภาษีไปแล้ว 5       เปิดเปิดเปิด...</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>เสียภาษีก้อยอมรวมอยู่ไหนไม่ถึง # ลอรีเอะซูเปอร...</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neg</td>\n",
       "      <td>primerของrevlon 22องศา เพิ่มเติมค่ะ</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neg</td>\n",
       "      <td>เบียร์ช้างขวดละ 10,0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neg</td>\n",
       "      <td>??เอาไหม จัดไปได้ดี ไทก่อนเรา</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos</td>\n",
       "      <td>ไม่รุ้ว่าสอยเหนตัวไหนน้องซีใครบ้างยอม1,0ฟีฟีฟี...</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neg</td>\n",
       "      <td>ดูสถานะการณ์ที่ตรงกัน ว่าจะร้านอาหารแต่</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              texts processed  wc  \\\n",
       "0      pos                     ก็น่าจะอร่อยกว่าบาบีก้อนอีกเลย             5   \n",
       "1      neg  อ่านเม้นดิน.มีปัญหามีปัญหามีปัญหานะนะนะ   คุณค...            13   \n",
       "2      neg             โบกแท็กซี่สิบคันกินข้าวกับเพื่อนไม่ได้             5   \n",
       "3      neg  ท่าทางยาบ้าเสียภาษีไปแล้ว 5       เปิดเปิดเปิด...            12   \n",
       "4      neg  เสียภาษีก้อยอมรวมอยู่ไหนไม่ถึง # ลอรีเอะซูเปอร...            15   \n",
       "5      neg                primerของrevlon 22องศา เพิ่มเติมค่ะ             6   \n",
       "6      neg                               เบียร์ช้างขวดละ 10,0             5   \n",
       "7      neg                      ??เอาไหม จัดไปได้ดี ไทก่อนเรา            10   \n",
       "8      pos  ไม่รุ้ว่าสอยเหนตัวไหนน้องซีใครบ้างยอม1,0ฟีฟีฟี...            19   \n",
       "9      neg            ดูสถานะการณ์ที่ตรงกัน ว่าจะร้านอาหารแต่             7   \n",
       "\n",
       "   uwc  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "5    0  \n",
       "6    0  \n",
       "7    0  \n",
       "8    0  \n",
       "9    0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean out tokens\n",
    "fake_df[\"texts\"] = fake_df.texts.map(lambda x: x.replace(\"xxbos\", \"\"))\n",
    "fake_df = fake_df[[\"category\", \"texts\", \"processed\", \"wc\", \"uwc\"]]\n",
    "fake_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ก็น่าจะอร่อยกว่าบาบีก้อนอีกเลย\n",
      "1 อ่านเม้นดิน.มีปัญหามีปัญหามีปัญหานะนะนะ   คุณคุณคุณทรายทรายทราย   สินสินสินชัยชัยชัยทองทองทอง   \n",
      "2 โบกแท็กซี่สิบคันกินข้าวกับเพื่อนไม่ได้\n",
      "3 ท่าทางยาบ้าเสียภาษีไปแล้ว 5       เปิดเปิดเปิดเปิดเปิดเปิดเปิดด่านด่านด่านด่านด่านด่านด่านทดสอบทดสอบทดสอบทดสอบทดสอบทดสอบทดสอบออนไลน์ออนไลน์ออนไลน์ออนไลน์ออนไลน์ออนไลน์ออนไลน์\n",
      "4 เสียภาษีก้อยอมรวมอยู่ไหนไม่ถึง # ลอรีเอะซูเปอร์อัลตร้าสลิมสลิม\n",
      "5 primerของrevlon 22องศา เพิ่มเติมค่ะ\n",
      "6 เบียร์ช้างขวดละ 10,0\n",
      "7 ??เอาไหม จัดไปได้ดี ไทก่อนเรา\n",
      "8 ไม่รุ้ว่าสอยเหนตัวไหนน้องซีใครบ้างยอม1,0ฟีฟีฟีๆๆๆ444   เพื่อนเพื่อนเพื่อน\n",
      "9 ดูสถานะการณ์ที่ตรงกัน ว่าจะร้านอาหารแต่\n",
      "10 บะหมี่เป็ดเอ็มเค หมูสไลด์เป็นเมนูไทยมากค่ะ สาขา\n",
      "11 บทเรียนนี้สอนให้รู้ว่าคนที่เล่นเกมส์นัดต่อไปใช้สิทธิ66 เพื่อไม่ให้เกิด\n",
      "12 ให้ก็ไม่อยากได้ กินไม่เวอร์ เพราะเป็นตับ\n",
      "13 ไปจ้าอาหื้มจะไปไหนดี106\n",
      "14 สนไหมมากินt-bone อร่อยกว่า\n",
      "15 ใครมาก็ได้ ลอรีเอะ ซูเปอร์ เจนเทิล เนื้อครีมครีม แสงคิ้ว \n",
      "16 เรโซน่าของออฟออฟยังไม่\n",
      "17 รถไฟฟ้าวันทำงานวันไหนดีจิงๆ # เดินเก็บไว้ngansaisilp\n",
      "18 ต้องไปรีบบอกงานใหม่มูลค่า 2,0   รับรับรับ\n",
      "19 ตัเงแต่แดกมอเตอร์อีเกินแล้วจะอ้วกยังหรอ ไปสักครั้งดิ\n"
     ]
    }
   ],
   "source": [
    "for i in range(20): print(i, fake_df.iloc[i,:].texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33482, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_aug = pd.read_csv(\"all_aug.csv\")\n",
    "fake_aug = pd.concat([all_aug,fake_df],0).reset_index(drop=True)\n",
    "fake_aug.to_csv(\"fake_aug.csv\", index=False)\n",
    "fake_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    17039\n",
       "neg     9782\n",
       "pos     6158\n",
       "q        503\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevalence\n",
    "fake_aug.category.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
